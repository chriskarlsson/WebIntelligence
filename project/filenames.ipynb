{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSA file name classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "for file_path in glob.glob('./*.txt'):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as source:\n",
    "        with open('normal_' + os.path.basename(file_path), 'a') as target:\n",
    "            for line in source:\n",
    "                new_line = ''\n",
    "                char = ' '\n",
    "                for next_char in line:\n",
    "                    if char.isdigit() and next_char.isdigit():\n",
    "                        new_line  += next_char\n",
    "                    elif char.isalpha() and next_char.isalpha() and not (char.islower() and next_char.isupper()):\n",
    "                        new_line += next_char\n",
    "                    elif next_char.isdigit() or next_char.isalpha():\n",
    "                        new_line += ' ' + next_char\n",
    "                    char = next_char\n",
    "                try:\n",
    "                    target.write(new_line.strip().lower() + '\\n')\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normal_csa.txt', 'r') as file:\n",
    "    csa_filenames = file.read().splitlines()\n",
    "\n",
    "with open('normal_home.txt', 'r') as file:\n",
    "    home_filenames = file.read().splitlines()\n",
    "    \n",
    "with open('normal_external_drive.txt', 'r') as file:\n",
    "    external_drive_filenames = file.read().splitlines()\n",
    "    \n",
    "with open('normal_nsrl.txt', 'r') as file:\n",
    "    nsrl_filenames = file.read().splitlines()[0::50] # Only take every 50th to not skew the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filenames = []\n",
    "target = []\n",
    "\n",
    "filenames.extend(csa_filenames)\n",
    "target.extend([0] * len(csa_filenames))\n",
    "\n",
    "filenames.extend(home_filenames)\n",
    "target.extend([1] * len(home_filenames))\n",
    "\n",
    "filenames.extend(external_drive_filenames)\n",
    "target.extend([1] * len(external_drive_filenames))\n",
    "\n",
    "filenames.extend(nsrl_filenames)\n",
    "target.extend([1] * len(nsrl_filenames))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(filenames, \n",
    "                                                    target, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=109)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median, mean\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify filenames using NB and only CSA for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All words\n",
      "Title\t\tMedian\t\t\tMean:\t\tMax:\n",
      "CSA test\t9.32773214538338e-10\t0.0001193148\t1.000000\n",
      "CSA all \t1.345064768124739e-09\t0.0001007138\t1.000000\n",
      "Home dir\t6.808818812584162e-20\t0.0000009497\t0.007432\n",
      "External\t1.8364258726484017e-14\t0.0000221181\t0.089162\n",
      "NSRL list\t6.489343397337807e-20\t0.0000276151\t0.023117\n",
      "\n",
      "Skip singles\n",
      "Title\t\tMedian\t\t\tMean:\t\tMax:\n",
      "CSA test\t6.632623725765875e-10\t0.0001193072\t1.000000\n",
      "CSA all \t9.607419179837564e-10\t0.0001006755\t1.000000\n",
      "Home dir\t5.902072501651128e-20\t0.0000009489\t0.007432\n",
      "External\t1.7179839371188966e-14\t0.0000221180\t0.089162\n",
      "NSRL list\t4.7275307102777694e-20\t0.0000276128\t0.023117\n",
      "\n",
      "Skip single and doubles\n",
      "Title\t\tMedian\t\t\tMean:\t\tMax:\n",
      "CSA test\t5.255420704766012e-10\t0.0001193019\t1.000000\n",
      "CSA all \t6.50305337159031e-10\t0.0001006617\t1.000000\n",
      "Home dir\t5.902072501651128e-20\t0.0000009480\t0.007432\n",
      "External\t1.2010584306914509e-14\t0.0000221178\t0.089162\n",
      "NSRL list\t3.55298891890441e-20\t0.0000276027\t0.023117\n",
      "\n",
      "Skip 10 most common\n",
      "Title\t\tMedian\t\t\tMean:\t\tMax:\n",
      "CSA test\t5.016762859518829e-12\t0.0000963088\t0.822201\n",
      "CSA all \t1.303003168739528e-11\t0.0000793694\t0.827459\n",
      "Home dir\t5.0560279928735285e-14\t0.0000436754\t0.313906\n",
      "External\t2.2615808738140764e-15\t0.0000105577\t0.057322\n",
      "NSRL list\t2.949253587275485e-14\t0.0014589704\t1.000005\n"
     ]
    }
   ],
   "source": [
    "class NBClassifier:\n",
    "    def __init__(self, word_count_dict):\n",
    "        self.word_count_dict = word_count_dict\n",
    "        self.normalizer = max(word_count_dict.values())\n",
    "        \n",
    "    def get_nb_scores(self, string_array):\n",
    "        result = []\n",
    "        for string in string_array:\n",
    "            result.append(self.get_nb_score(string))\n",
    "        return result\n",
    "\n",
    "    def get_nb_score(self, string):\n",
    "        score = 1\n",
    "        count = 0\n",
    "        for word in string.split(' '):\n",
    "            score *= (self.word_count_dict.get(word, 0) + 0.1) / self.normalizer\n",
    "            count += 1\n",
    "        return score / count\n",
    "\n",
    "def print_information(title, scores):\n",
    "    print('{}\\t{}\\t{:.10f}\\t{:.6f}'.format(title, median(scores), mean(scores), max(scores)))\n",
    "    \n",
    "def calc_and_print(word_count_dict):\n",
    "    nb_csa_clf = NBClassifier(word_count_dict)\n",
    "    print('Title\\t\\tMedian\\t\\t\\tMean:\\t\\tMax:')\n",
    "    print_information('CSA test', nb_csa_clf.get_nb_scores(csa_filenames_test))\n",
    "    print_information('CSA all ', nb_csa_clf.get_nb_scores(csa_filenames))\n",
    "    print_information('Home dir', nb_csa_clf.get_nb_scores(home_filenames))\n",
    "    print_information('External', nb_csa_clf.get_nb_scores(external_drive_filenames))\n",
    "    print_information('NSRL list', nb_csa_clf.get_nb_scores(nsrl_filenames))\n",
    "\n",
    "print('\\nAll words')\n",
    "calc_and_print(word_counts)\n",
    "print('\\nSkip singles')\n",
    "calc_and_print({key: value for key, value in word_counts.items() if value > 1 })\n",
    "print('\\nSkip single and doubles')\n",
    "calc_and_print({key: value for key, value in word_counts.items() if value > 2 })\n",
    "print('\\nSkip 10 most common')\n",
    "tenth_most_common = sorted(word_counts.values(), reverse=True)[10]\n",
    "calc_and_print({key: value for key, value in word_counts.items() if value <  tenth_most_common})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1, 1e-1, 1e-2, 1e-3)}\n",
    "              \n",
    "nb_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "nb_gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=-1, cv=3)\n",
    "nb_gs_clf.fit(X_train, y_train)\n",
    "\n",
    "print(nb_gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709848356533252\n",
      "0.9953585614100415\n",
      "0.0038658161737064193\n",
      "0.23845101793599313\n",
      "0.017322799240090052\n"
     ]
    }
   ],
   "source": [
    "print(nb_gs_clf.score(X_test, y_test))\n",
    "print(nb_gs_clf.score(csa_filenames, [0]*len(csa_filenames)))\n",
    "print(nb_gs_clf.score(home_filenames, [0]*len(home_filenames)))\n",
    "print(nb_gs_clf.score(external_drive_filenames, [0]*len(external_drive_filenames)))\n",
    "print(nb_gs_clf.score(nsrl_filenames, [0]*len(nsrl_filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip..._state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf-svm', SGDClassifier(loss='hinge', tol=0.001))])\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649737121366623\n",
      "0.9903610438776144\n",
      "0.002830509190108496\n",
      "0.5168926575749286\n",
      "0.024250488228855935\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_test, y_test))\n",
    "print(svm_clf.score(csa_filenames, [0]*len(csa_filenames)))\n",
    "print(svm_clf.score(home_filenames, [0]*len(home_filenames)))\n",
    "print(svm_clf.score(external_drive_filenames, [0]*len(external_drive_filenames)))\n",
    "print(svm_clf.score(nsrl_filenames, [0]*len(nsrl_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf-svm__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf-svm', SGDClassifier(loss='hinge', tol=0.001))])\n",
    "\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__alpha': (1, 1e-1, 1e-2)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(svm_clf, parameters_svm, n_jobs=-1, cv=3)\n",
    "gs_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9288896877603293\n",
      "0.9315893998634642\n",
      "0.0035662807551509034\n",
      "0.7203079207533899\n",
      "0.025893757721987008\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf_svm.score(X_test, y_test))\n",
    "print(gs_clf_svm.score(csa_filenames, [0]*len(csa_filenames)))\n",
    "print(gs_clf_svm.score(home_filenames, [0]*len(home_filenames)))\n",
    "print(gs_clf_svm.score(external_drive_filenames, [0]*len(external_drive_filenames)))\n",
    "print(gs_clf_svm.score(nsrl_filenames, [0]*len(nsrl_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
